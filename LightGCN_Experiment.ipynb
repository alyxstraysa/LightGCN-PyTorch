{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LightGCN Experiment",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCQJ4gNHoCIiadL13cmekx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alyxstraysa/LightGCN-PyTorch/blob/master/LightGCN_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFlV1rhIExxB",
        "outputId": "845b087b-ca1a-455a-ccd4-6a03774b02df"
      },
      "source": [
        "! git clone https://github.com/alyxstraysa/LightGCN-PyTorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LightGCN-PyTorch'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 533 (delta 25), reused 30 (delta 11), pack-reused 477\u001b[K\n",
            "Receiving objects: 100% (533/533), 80.23 MiB | 36.81 MiB/s, done.\n",
            "Resolving deltas: 100% (310/310), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86Fln3WqVxLF",
        "outputId": "67bca144-d61d-4518-b684-bfacf2ecf45e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50mWdas-auqw",
        "outputId": "0afb2559-182c-42d6-b340-87bbbc6b70ac"
      },
      "source": [
        "cd /content/LightGCN-PyTorch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LightGCN-PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bLd9ON1ZayFt",
        "outputId": "55774e4b-1093-4a85-de29-ecb8fae7b99b"
      },
      "source": [
        "pip install -r requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 23kB/s \n",
            "\u001b[?25hCollecting pandas==0.24.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 56.8MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 92kB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 125kB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 56.0MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 40.5MB/s \n",
            "\u001b[?25hCollecting tqdm==4.48.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2->-r requirements.txt (line 6)) (0.17.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->-r requirements.txt (line 5)) (50.3.2)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.3.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, numpy, pandas, scipy, tensorboardX, threadpoolctl, scikit-learn, tqdm\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: pandas 1.1.4\n",
            "    Uninstalling pandas-1.1.4:\n",
            "      Successfully uninstalled pandas-1.1.4\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed numpy-1.16.4 pandas-0.24.2 scikit-learn-0.23.2 scipy-1.3.0 tensorboardX-1.8 threadpoolctl-2.1.0 torch-1.4.0 tqdm-4.48.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dycqn8rwE8eD",
        "outputId": "46dfc582-d123-45b0-b142-f7ee256bef94"
      },
      "source": [
        "cd /content/LightGCN-PyTorch/code/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LightGCN-PyTorch/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nhg7e3xFvTF"
      },
      "source": [
        "#/content/LightGCN-PyTorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJJHFigXFclq",
        "outputId": "f0dee188-6ff7-4e3d-b3f1-d62b972e40d2"
      },
      "source": [
        "! python main.py --decay=1e-4 --lr=0.001 --layer=3 --seed=2020 --epochs 100 --dataset=\"anime\" --topks=\"[20]\" --recdim=64 --testbatch==1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0;30;43mCpp extension not loaded\u001b[0m\n",
            ">>SEED: 2020\n",
            "\u001b[0;30;43mloading [../data/anime]\u001b[0m\n",
            "13065 interactions for training\n",
            "1780 interactions for testing\n",
            "anime Sparsity : 0.023852102503787897\n",
            "anime is ready to go\n",
            "===========config================\n",
            "{'A_n_fold': 100,\n",
            " 'A_split': False,\n",
            " 'bigdata': False,\n",
            " 'bpr_batch_size': 2048,\n",
            " 'decay': 0.0001,\n",
            " 'dropout': 0,\n",
            " 'keep_prob': 0.6,\n",
            " 'latent_dim_rec': 64,\n",
            " 'lightGCN_n_layers': 3,\n",
            " 'lr': 0.001,\n",
            " 'multicore': 0,\n",
            " 'pretrain': 0,\n",
            " 'test_u_batch_size': 100}\n",
            "cores for test: 1\n",
            "comment: lgn\n",
            "tensorboard: 1\n",
            "LOAD: 0\n",
            "Weight path: ./checkpoints\n",
            "Test Topks: [20]\n",
            "using bpr loss\n",
            "===========end===================\n",
            "\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n",
            "loading adjacency matrix\n",
            "generating adjacency matrix\n",
            "/content/LightGCN-PyTorch/code/dataloader.py:351: RuntimeWarning: divide by zero encountered in power\n",
            "  d_inv = np.power(rowsum, -0.5).flatten()\n",
            "costing 0.09539079666137695s, saved norm_mat...\n",
            "don't split the matrix\n",
            "lgn is already to go(dropout:0)\n",
            "load and save to /content/LightGCN-PyTorch/code/checkpoints/lgn-anime-3-64.pth.tar\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.05625]), 'recall': array([0.0042156]), 'ndcg': array([0.05518624])}\n",
            "EPOCH[1/100] loss0.691-|Sample:0.04|\n",
            "EPOCH[2/100] loss0.690-|Sample:0.07|\n",
            "EPOCH[3/100] loss0.690-|Sample:0.04|\n",
            "EPOCH[4/100] loss0.688-|Sample:0.05|\n",
            "EPOCH[5/100] loss0.687-|Sample:0.04|\n",
            "EPOCH[6/100] loss0.686-|Sample:0.04|\n",
            "EPOCH[7/100] loss0.684-|Sample:0.04|\n",
            "EPOCH[8/100] loss0.682-|Sample:0.04|\n",
            "EPOCH[9/100] loss0.679-|Sample:0.04|\n",
            "EPOCH[10/100] loss0.677-|Sample:0.04|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.05]), 'recall': array([0.00398906]), 'ndcg': array([0.04979968])}\n",
            "EPOCH[11/100] loss0.674-|Sample:0.04|\n",
            "EPOCH[12/100] loss0.669-|Sample:0.05|\n",
            "EPOCH[13/100] loss0.666-|Sample:0.05|\n",
            "EPOCH[14/100] loss0.661-|Sample:0.05|\n",
            "EPOCH[15/100] loss0.656-|Sample:0.04|\n",
            "EPOCH[16/100] loss0.650-|Sample:0.05|\n",
            "EPOCH[17/100] loss0.643-|Sample:0.04|\n",
            "EPOCH[18/100] loss0.637-|Sample:0.04|\n",
            "EPOCH[19/100] loss0.629-|Sample:0.04|\n",
            "EPOCH[20/100] loss0.621-|Sample:0.05|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.05]), 'recall': array([0.00398906]), 'ndcg': array([0.04534896])}\n",
            "EPOCH[21/100] loss0.612-|Sample:0.04|\n",
            "EPOCH[22/100] loss0.603-|Sample:0.04|\n",
            "EPOCH[23/100] loss0.595-|Sample:0.04|\n",
            "EPOCH[24/100] loss0.584-|Sample:0.04|\n",
            "EPOCH[25/100] loss0.577-|Sample:0.04|\n",
            "EPOCH[26/100] loss0.566-|Sample:0.04|\n",
            "EPOCH[27/100] loss0.551-|Sample:0.04|\n",
            "EPOCH[28/100] loss0.539-|Sample:0.05|\n",
            "EPOCH[29/100] loss0.526-|Sample:0.04|\n",
            "EPOCH[30/100] loss0.523-|Sample:0.04|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.0625]), 'recall': array([0.00724861]), 'ndcg': array([0.05620818])}\n",
            "EPOCH[31/100] loss0.506-|Sample:0.04|\n",
            "EPOCH[32/100] loss0.501-|Sample:0.05|\n",
            "EPOCH[33/100] loss0.488-|Sample:0.04|\n",
            "EPOCH[34/100] loss0.468-|Sample:0.04|\n",
            "EPOCH[35/100] loss0.464-|Sample:0.04|\n",
            "EPOCH[36/100] loss0.458-|Sample:0.04|\n",
            "EPOCH[37/100] loss0.450-|Sample:0.04|\n",
            "EPOCH[38/100] loss0.448-|Sample:0.04|\n",
            "EPOCH[39/100] loss0.438-|Sample:0.04|\n",
            "EPOCH[40/100] loss0.432-|Sample:0.04|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.1]), 'recall': array([0.01712957]), 'ndcg': array([0.08875816])}\n",
            "EPOCH[41/100] loss0.411-|Sample:0.04|\n",
            "EPOCH[42/100] loss0.398-|Sample:0.05|\n",
            "EPOCH[43/100] loss0.399-|Sample:0.04|\n",
            "EPOCH[44/100] loss0.389-|Sample:0.04|\n",
            "EPOCH[45/100] loss0.391-|Sample:0.05|\n",
            "EPOCH[46/100] loss0.379-|Sample:0.05|\n",
            "EPOCH[47/100] loss0.385-|Sample:0.04|\n",
            "EPOCH[48/100] loss0.376-|Sample:0.05|\n",
            "EPOCH[49/100] loss0.376-|Sample:0.04|\n",
            "EPOCH[50/100] loss0.372-|Sample:0.04|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.10625]), 'recall': array([0.01754623]), 'ndcg': array([0.09834069])}\n",
            "EPOCH[51/100] loss0.352-|Sample:0.04|\n",
            "EPOCH[52/100] loss0.370-|Sample:0.04|\n",
            "EPOCH[53/100] loss0.362-|Sample:0.05|\n",
            "EPOCH[54/100] loss0.370-|Sample:0.04|\n",
            "EPOCH[55/100] loss0.348-|Sample:0.05|\n",
            "EPOCH[56/100] loss0.347-|Sample:0.04|\n",
            "EPOCH[57/100] loss0.357-|Sample:0.04|\n",
            "EPOCH[58/100] loss0.342-|Sample:0.04|\n",
            "EPOCH[59/100] loss0.329-|Sample:0.04|\n",
            "EPOCH[60/100] loss0.335-|Sample:0.05|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.10625]), 'recall': array([0.01754623]), 'ndcg': array([0.10073631])}\n",
            "EPOCH[61/100] loss0.331-|Sample:0.04|\n",
            "EPOCH[62/100] loss0.336-|Sample:0.05|\n",
            "EPOCH[63/100] loss0.348-|Sample:0.05|\n",
            "EPOCH[64/100] loss0.332-|Sample:0.04|\n",
            "EPOCH[65/100] loss0.327-|Sample:0.04|\n",
            "EPOCH[66/100] loss0.325-|Sample:0.05|\n",
            "EPOCH[67/100] loss0.322-|Sample:0.05|\n",
            "EPOCH[68/100] loss0.297-|Sample:0.05|\n",
            "EPOCH[69/100] loss0.318-|Sample:0.05|\n",
            "EPOCH[70/100] loss0.314-|Sample:0.07|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.1125]), 'recall': array([0.01873671]), 'ndcg': array([0.10505352])}\n",
            "EPOCH[71/100] loss0.316-|Sample:0.04|\n",
            "EPOCH[72/100] loss0.305-|Sample:0.04|\n",
            "EPOCH[73/100] loss0.303-|Sample:0.04|\n",
            "EPOCH[74/100] loss0.323-|Sample:0.04|\n",
            "EPOCH[75/100] loss0.299-|Sample:0.04|\n",
            "EPOCH[76/100] loss0.299-|Sample:0.04|\n",
            "EPOCH[77/100] loss0.297-|Sample:0.04|\n",
            "EPOCH[78/100] loss0.294-|Sample:0.04|\n",
            "EPOCH[79/100] loss0.307-|Sample:0.05|\n",
            "EPOCH[80/100] loss0.308-|Sample:0.04|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.1125]), 'recall': array([0.01951052]), 'ndcg': array([0.10562571])}\n",
            "EPOCH[81/100] loss0.308-|Sample:0.04|\n",
            "EPOCH[82/100] loss0.300-|Sample:0.05|\n",
            "EPOCH[83/100] loss0.308-|Sample:0.04|\n",
            "EPOCH[84/100] loss0.302-|Sample:0.04|\n",
            "EPOCH[85/100] loss0.302-|Sample:0.05|\n",
            "EPOCH[86/100] loss0.305-|Sample:0.04|\n",
            "EPOCH[87/100] loss0.284-|Sample:0.04|\n",
            "EPOCH[88/100] loss0.306-|Sample:0.04|\n",
            "EPOCH[89/100] loss0.280-|Sample:0.04|\n",
            "EPOCH[90/100] loss0.286-|Sample:0.04|\n",
            "\u001b[0;30;43m[TEST]\u001b[0m\n",
            "test_u_batch_size is too big for this dataset, try a small one 0\n",
            "{'precision': array([0.10625]), 'recall': array([0.01326052]), 'ndcg': array([0.10079556])}\n",
            "EPOCH[91/100] loss0.279-|Sample:0.04|\n",
            "EPOCH[92/100] loss0.294-|Sample:0.04|\n",
            "EPOCH[93/100] loss0.296-|Sample:0.05|\n",
            "EPOCH[94/100] loss0.283-|Sample:0.04|\n",
            "EPOCH[95/100] loss0.283-|Sample:0.04|\n",
            "EPOCH[96/100] loss0.285-|Sample:0.04|\n",
            "EPOCH[97/100] loss0.283-|Sample:0.04|\n",
            "EPOCH[98/100] loss0.270-|Sample:0.04|\n",
            "EPOCH[99/100] loss0.287-|Sample:0.04|\n",
            "EPOCH[100/100] loss0.275-|Sample:0.04|\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}